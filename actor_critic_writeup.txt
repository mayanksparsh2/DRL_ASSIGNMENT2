The Actor-Critic agent was trained on a dataset of ICU stays to learn treatment policies. Rewards are computed as the negative squared deviation of vital signs from healthy targets. During training the agent experiences sequences that represent individual ICU episodes. The policy and value networks are updated using the advantage between returns and the critic's value estimates. Although the dataset-driven environment does not react to the agent's chosen action, the training loop demonstrates how an on-policy method can be implemented. Across the short training run, episode rewards remain fairly stable, reflecting the deterministic nature of replayed data. Longer training with more expressive models and interaction with a responsive simulator would allow the agent to adaptively choose interventions, leading to more pronounced reward trends and the potential for policy improvement. Dynamic reward graphs are generated and saved as animated GIFs to illustrate training progress.
